{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_TEMPLATE = \"https://www.idntimes.com/tag/pelecehan-seksual?sort=desc&page={}\"\n",
    "\n",
    "def get_html_content(url):\n",
    "  html_content = requests.get(url).text\n",
    "  soup = BeautifulSoup(html_content, \"lxml\")\n",
    "  return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_urls(count):\n",
    "    urls = []\n",
    "    if count < 1:\n",
    "        return urls\n",
    "\n",
    "    page = 1\n",
    "    while True:\n",
    "        soup = get_html_content(URL_TEMPLATE.format(page))\n",
    "        contents = soup.find_all(\"a\", class_=\"article__link\")  # get article list\n",
    "        for content in contents:\n",
    "            if len(urls) == count:\n",
    "                return urls\n",
    "            urls.append(content.attrs['href'])\n",
    "\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_by_date(date_str):\n",
    "    urls = []\n",
    "    page = 1\n",
    "    target_date = datetime.strptime(date_str, \"%d/%m/%Y\")\n",
    "    is_end = False\n",
    "\n",
    "\n",
    "    while not is_end:\n",
    "        soup = get_html_content(URL_TEMPLATE.format(page))\n",
    "        contents = soup.find_all(\"div\", class_=[\"article__box\", \"article__list\"])  # get article list\n",
    "        for content in contents:\n",
    "            news_datetime_str = content.find(\"div\", class_=\"article__date\").text\n",
    "            news_date_str = news_datetime_str.split(',')[0]\n",
    "            news_date = datetime.strptime(news_date_str, \"%d/%m/%Y\")\n",
    "\n",
    "            ## Compare the current article with target date\n",
    "            if news_date.date() > target_date.date():\n",
    "                break\n",
    "            if news_date.date() < target_date.date():\n",
    "                is_end = True\n",
    "                break\n",
    "            if news_date.date() == target_date.date():\n",
    "                urls.append(content.find('a').attrs['href'])\n",
    "\n",
    "        page += 1\n",
    "        \n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_attr(url):\n",
    "    page = get_html_content(url)\n",
    "    title = page.find(\"h1\", class_=\"read__title\").text\n",
    "\n",
    "    ## div format: Kompas.com - 06/05/2021, 13:40 WIB\n",
    "    datetime_str = page.find('div', class_=\"read__time\").text.split('-')[1].strip()\n",
    "    date_str = datetime_str.split(',')[0]\n",
    "\n",
    "    img_content = page.find('div', class_=\"photo__wrap\")\n",
    "    img_link= img_content.find('img').attrs['src']\n",
    "\n",
    "    contents = page.find('div', class_=\"read__content\")\n",
    "    paragraphs = contents.find_all('p')\n",
    "\n",
    "    news_content = {}\n",
    "    i = 0\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph = unicodedata.normalize(\"NFKD\", paragraph.text).strip()\n",
    "\n",
    "        # Skip empty and 'baca juga' paragraph\n",
    "        if paragraph != \"\" and not paragraph.startswith(\"Baca juga:\"):\n",
    "            news_content[i] = paragraph\n",
    "            i += 1\n",
    "\n",
    "    tags = page.find_all('li', class_=\"tag__article__item\")\n",
    "    tags_list = []\n",
    "    for tag in tags:\n",
    "       tags_list.append(tag.text)\n",
    "\n",
    "    return {\n",
    "        'portal': 'idn.com',\n",
    "        'date': date_str,\n",
    "        'link': url,\n",
    "        'title': title,\n",
    "        'image': img_link,\n",
    "        'content': news_content,\n",
    "        'tags': tags_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idn_dataframe_from_date(date):\n",
    "    url_list = get_urls_by_date(date)\n",
    "    all_data = []\n",
    "    for url in url_list:\n",
    "        all_data += [get_news_attr(url  + '?page=all' )]\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ## Scrap by date example\n",
    "    date = '09/05/2022'\n",
    "    urls = get_urls_by_date(date)\n",
    "    news_attr = get_news_attr(urls[0] + '?page=all')\n",
    "    print(news_attr)\n",
    "\n",
    "    ## Scrap n news example\n",
    "    urls = get_n_urls(10)\n",
    "    df = pd.DataFrame(urls, columns=['Link'])\n",
    "    df.to_csv('idn.csv', sep = ',', index = False, quoting=csv.QUOTE_ALL) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b51345ca610e7a8404e7d5913c9c9082c26b7cc9f82c0fb2e4006519af425d45"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
