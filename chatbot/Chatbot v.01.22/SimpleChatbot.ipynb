{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, let me explained this chatbot. \n",
    "\n",
    "Why i'm rename Simple Chatbot? because this ChatBot just only using Clasification Concept. We have list of input text conversation and output conversation. This chatbot using data history in dataset, so if this Chatbot looking sentences not in training, it cann't detection. I'm not sure this chatbot can be responses using new data from user, but if we want use like that, we can using Generate Chatbot. This my argument has I know, but if you have more knowladge, let's discussion soon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('dataset/humanText.json') as data_file:\n",
    "    data = json.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textInput</th>\n",
       "      <th>intents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Halo Ara</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Halo</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halo</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selamat Pagi</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Selamat Siang</td>\n",
       "      <td>salam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textInput intents\n",
       "0       Halo Ara   salam\n",
       "1           Halo   salam\n",
       "2           Halo   salam\n",
       "3   Selamat Pagi   salam\n",
       "4  Selamat Siang   salam"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textInput = []\n",
    "humanText = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        textInput.append(pattern)\n",
    "        humanText.append(intent['tag'])\n",
    "\n",
    "df = pd.DataFrame({'textInput': textInput,\n",
    "                    'intents': humanText})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kronologi             83\n",
       "laporOrang            70\n",
       "usiaPelaku            40\n",
       "pekerjaan             33\n",
       "salam                 28\n",
       "hubungan              27\n",
       "tempat                27\n",
       "laporDiri             22\n",
       "pendidikanKorban      20\n",
       "adaKorbanLain         14\n",
       "pendidikanPelaku      11\n",
       "bye                   11\n",
       "korbanLain             8\n",
       "tidakadaKorbanLain     7\n",
       "Name: intents, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.intents.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleansing\n",
    "import string\n",
    "\n",
    "# convert lowercase\n",
    "df.textInput = df.textInput.apply(lambda x: x.lower())\n",
    "\n",
    "# remove punctuation\n",
    "exclude = set(string.punctuation)\n",
    "df.textInput = df.textInput.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding to labelling tag category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df.intents)\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab = []\n",
    "length = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    sent = row['textInput']\n",
    "    [all_vocab.append(i) for i in sent.split()]\n",
    "    length.append(len(sent.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2538"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(all_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization to convert sentences be vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "max_vocab_length = 462\n",
    "max_length = 152\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                       standardize='lower_and_strip_punctuation',\n",
    "                                       split='whitespace',\n",
    "                                       ngrams=None,\n",
    "                                       output_mode='int',\n",
    "                                       output_sequence_length=max_length\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(df.textInput)\n",
    "#text_vectorization.get_vocabulary()\n",
    "#text_vectorization('halo ara ingin membantu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding to create arrary from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 16), dtype=float32, numpy=\n",
       "array([[[ 0.04252667, -0.01084045, -0.01178681,  0.03017611,\n",
       "         -0.00412071, -0.02765962,  0.03103551,  0.02681298,\n",
       "         -0.04006124,  0.02020841,  0.01969254, -0.04620464,\n",
       "         -0.03793932, -0.03745115,  0.00915391, -0.01976777],\n",
       "        [-0.01660417, -0.00672623,  0.00442706, -0.04289648,\n",
       "         -0.04893948, -0.02255445,  0.01061283,  0.0231486 ,\n",
       "          0.04656494, -0.01453026, -0.01130132,  0.01887038,\n",
       "         -0.00403232, -0.01606822, -0.04683238, -0.0449682 ],\n",
       "        [ 0.04138238, -0.00832059, -0.00115852,  0.01247829,\n",
       "          0.02694828,  0.04907492,  0.03925398,  0.03525894,\n",
       "         -0.04290357, -0.04658737, -0.03474482,  0.00431371,\n",
       "          0.03371746, -0.04131329,  0.0385217 , -0.04083408],\n",
       "        [ 0.04507855, -0.01723065,  0.00276566, -0.00999359,\n",
       "         -0.00655423,  0.04067147, -0.04068791, -0.0257197 ,\n",
       "          0.03716056, -0.03221033,  0.03156299,  0.0385995 ,\n",
       "          0.00235199, -0.00343263, -0.00666933,  0.01496159],\n",
       "        [-0.03216155, -0.02432609, -0.02032104, -0.00708147,\n",
       "         -0.01494852,  0.00564519, -0.00337347, -0.0126196 ,\n",
       "         -0.0025359 , -0.04650595, -0.01591195, -0.02207941,\n",
       "          0.01415769, -0.01674174, -0.0252015 , -0.01376449],\n",
       "        [-0.03216155, -0.02432609, -0.02032104, -0.00708147,\n",
       "         -0.01494852,  0.00564519, -0.00337347, -0.0126196 ,\n",
       "         -0.0025359 , -0.04650595, -0.01591195, -0.02207941,\n",
       "          0.01415769, -0.01674174, -0.0252015 , -0.01376449]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "embedding = Embedding(input_dim=max_vocab_length,\n",
    "                      output_dim=16,\n",
    "                      embeddings_initializer=\"uniform\",\n",
    "                      input_length=max_length)\n",
    "import numpy as np\n",
    "res_embed = embedding(np.array([[71, 17,  7, 13,  0,  0]]))\n",
    "res_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "inputs = Input(shape=(1,), dtype='string')\n",
    "x = text_vectorization(inputs)\n",
    "x = embedding(x)\n",
    "x = LSTM(12)(x)\n",
    "outputs = Dense(13, activation='softmax')(x)\n",
    "modelLSTM = Model(inputs, outputs, name=\"LSTM_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "modelLSTM.compile(loss='categorical_crossentropy',\n",
    "                   optimizer='adam',\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 14) and (None, 13) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\melli\\OneDrive\\Documents\\GitHub\\C22-PS276-Iswara\\chatbot\\SimpleChatbot\\SimpleChatbot.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/melli/OneDrive/Documents/GitHub/C22-PS276-Iswara/chatbot/SimpleChatbot/SimpleChatbot.ipynb#ch0000026?line=0'>1</a>\u001b[0m modelLSTM\u001b[39m.\u001b[39;49mfit(df\u001b[39m.\u001b[39;49mtextInput,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/melli/OneDrive/Documents/GitHub/C22-PS276-Iswara/chatbot/SimpleChatbot/SimpleChatbot.ipynb#ch0000026?line=1'>2</a>\u001b[0m                y_train,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/melli/OneDrive/Documents/GitHub/C22-PS276-Iswara/chatbot/SimpleChatbot/SimpleChatbot.ipynb#ch0000026?line=2'>3</a>\u001b[0m                epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/melli/OneDrive/Documents/GitHub/C22-PS276-Iswara/chatbot/SimpleChatbot/SimpleChatbot.ipynb#ch0000026?line=3'>4</a>\u001b[0m                verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/melli/AppData/Local/Programs/Python/Python310/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\melli\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 14) and (None, 13) are incompatible\n"
     ]
    }
   ],
   "source": [
    "modelLSTM.fit(df.textInput,\n",
    "               y_train,\n",
    "               epochs=150,\n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.evaluate(df.textInput, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM.save(\"modelLSTM.tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "le_filename = open(\"label_encoder.pickle\", \"wb\")\n",
    "pickle.dump(le, le_filename)\n",
    "le_filename.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b51345ca610e7a8404e7d5913c9c9082c26b7cc9f82c0fb2e4006519af425d45"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
